![Hugging Face Transformers Library](https://camo.githubusercontent.com/19694a747faa4c55cbdb1cab99086099c6cf961930712f87ab3469e9bf706a4f/68747470733a2f2f68756767696e67666163652e636f2f64617461736574732f68756767696e67666163652f646f63756d656e746174696f6e2d696d616765732f7261772f6d61696e2f7472616e73666f726d6572732d6c6f676f2d6c696768742e737667)

### 源码学习

#### transformers的架构

[编码器-解码器架构](tutorials/encoding_decoding.md)

[自注意力机制](tutorials/self_attention.md)

[多头注意力机制](tutorials/multi_head_attention.md)

[位置编码](tutorials/positional_encoding.md)

#####  添加一个实践

[适配llama各类](./tutorials/transformers_model_code.md)

#### 预训练和微调

[预训练任务和目标]()

[预训练数据集]()

[微调策略和技巧]()

### 社区贡献

[领取新手任务](https://github.com/huggingface/transformers/contribute)

[领取进阶任务](https://github.com/huggingface/transformers/labels/Good%20Second%20Issue)

[如何创建第一个PR](https://github.com/huggingface/transformers/labels/Good%20Second%20Issue)

[如何贡献文档](https://github.com/huggingface/transformers/tree/main/docs)

[如何添加新模型](https://huggingface.co/docs/transformers/v4.40.2/en/add_new_model)

### 学习资源

[论坛解惑](https://discuss.huggingface.co/)

[Hugging Face NLP课程](https://huggingface.co/learn/nlp-course/chapter0/1?fw=pt)

[机器学习](tutorials/machine_learning.md)

[深度学习](tutorials/deep_learning.md)

[自然语言处理(NLP)](tutorials/natural_language_processing.md)
